{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/alex/Library/CloudStorage/OneDrive-MUNI/Dokumenty/Projects Data/NLP/SciDocs/'\n",
    "paper_cite_file = data_path + 'paper_metadata_view_cite_read.json'\n",
    "paper_cls_file = data_path + 'paper_metadata_mag_mesh.json'\n",
    "paper_rec_file = data_path + 'paper_metadata_recomm.json'\n",
    "user_activity_and_citations_embeddings_path = data_path + 'specter-embeddings/user-citation.jsonl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_data = {}\n",
    "with open(paper_rec_file, 'r') as f:\n",
    "    papers_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract', 'authors', 'cited_by', 'paper_id', 'references', 'title', 'year']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(papers_data.values())[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = pd.DataFrame(papers_data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>cited_by</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0002c0f45b3ef0f1491f91cbfefe9543e9af6163</th>\n",
       "      <td>In this paper we introduce an hp certified red...</td>\n",
       "      <td>[3122778, 39921175, 1905947, 2848614]</td>\n",
       "      <td>[16728d6f0a225bb8a71ebe4d0acd2512ca775327, 48d...</td>\n",
       "      <td>0002c0f45b3ef0f1491f91cbfefe9543e9af6163</td>\n",
       "      <td>[07e1f620f68c0be579fb05bf6d231fa06b0db7c3, 7f2...</td>\n",
       "      <td>An hp Certified Reduced Basis Method for Param...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004d38aa501306e2719e4d0413dcb5c788676b1</th>\n",
       "      <td>We test whether momentum strategies remain pro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[67d88e58812562f0270b7077408776157a542fbf, 946...</td>\n",
       "      <td>0004d38aa501306e2719e4d0413dcb5c788676b1</td>\n",
       "      <td>[18cb63580217983f2fd4b54141b2f83b96819dd3, 360...</td>\n",
       "      <td>Are Momentum Profits Robust to Trading Costs</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000587e08c6ce8c3d4c74360e34abe7c543a0e98</th>\n",
       "      <td>OBJECTIVES\\nThe death of a child in the pediat...</td>\n",
       "      <td>[4526222, 6110925, 38791518, 3743376, 31914233...</td>\n",
       "      <td>[55bac4ab517f5174d3259e7631d4fc6fa58cbac7, 593...</td>\n",
       "      <td>000587e08c6ce8c3d4c74360e34abe7c543a0e98</td>\n",
       "      <td>[95f677c6287e19a9afb8c25848b6f3437340e17a, ecb...</td>\n",
       "      <td>\"I was able to still be her mom\"--parenting at...</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   abstract  \\\n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163  In this paper we introduce an hp certified red...   \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1  We test whether momentum strategies remain pro...   \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  OBJECTIVES\\nThe death of a child in the pediat...   \n",
       "\n",
       "                                                                                    authors  \\\n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163              [3122778, 39921175, 1905947, 2848614]   \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1                                                 []   \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  [4526222, 6110925, 38791518, 3743376, 31914233...   \n",
       "\n",
       "                                                                                   cited_by  \\\n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163  [16728d6f0a225bb8a71ebe4d0acd2512ca775327, 48d...   \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1  [67d88e58812562f0270b7077408776157a542fbf, 946...   \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  [55bac4ab517f5174d3259e7631d4fc6fa58cbac7, 593...   \n",
       "\n",
       "                                                                          paper_id  \\\n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163  0002c0f45b3ef0f1491f91cbfefe9543e9af6163   \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1  0004d38aa501306e2719e4d0413dcb5c788676b1   \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  000587e08c6ce8c3d4c74360e34abe7c543a0e98   \n",
       "\n",
       "                                                                                 references  \\\n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163  [07e1f620f68c0be579fb05bf6d231fa06b0db7c3, 7f2...   \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1  [18cb63580217983f2fd4b54141b2f83b96819dd3, 360...   \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  [95f677c6287e19a9afb8c25848b6f3437340e17a, ecb...   \n",
       "\n",
       "                                                                                      title  \\\n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163  An hp Certified Reduced Basis Method for Param...   \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1       Are Momentum Profits Robust to Trading Costs   \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  \"I was able to still be her mom\"--parenting at...   \n",
       "\n",
       "                                          year  \n",
       "0002c0f45b3ef0f1491f91cbfefe9543e9af6163  2011  \n",
       "0004d38aa501306e2719e4d0413dcb5c788676b1  2003  \n",
       "000587e08c6ce8c3d4c74360e34abe7c543a0e98  2012  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extra column\n",
    "all(papers_df.index == papers_df.paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36261, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(papers_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_id, paper_attrs in papers_data.items():\n",
    "    for citing_id in paper_attrs['cited_by']:\n",
    "        if citing_id in G:\n",
    "            G.add_edge(citing_id, paper_id)\n",
    "    for cited_id in paper_attrs['references']:\n",
    "        if cited_id in G:\n",
    "            G.add_edge(paper_id, cited_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_comp_sizes = []\n",
    "for c in nx.weakly_connected_components(G):\n",
    "    conn_comp_sizes.append(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4716, 187, 111, 97, 93, 84, 61, 56, 55, 46]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(conn_comp_sizes, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_comps = {len(c):c for c in nx.weakly_connected_components(G)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_conn_comp = conn_comps[max(conn_comps.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4716"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(largest_conn_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glcc = G.subgraph(largest_conn_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4716"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Glcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine previous cells into functions\n",
    "def get_graph(file_name):\n",
    "    papers_data = {}\n",
    "    with open(file_name, 'r') as f:\n",
    "        papers_data = json.loads(f.read())\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(papers_data.keys())\n",
    "    for paper_id, paper_attrs in papers_data.items():\n",
    "        for citing_id in paper_attrs['cited_by']:\n",
    "            if citing_id in G:\n",
    "                G.add_edge(citing_id, paper_id)\n",
    "        for cited_id in paper_attrs['references']:\n",
    "            if cited_id in G:\n",
    "                G.add_edge(paper_id, cited_id)\n",
    "    return G\n",
    "\n",
    "def get_LCC(G):\n",
    "    conn_comps = {len(c):c for c in nx.weakly_connected_components(G)}\n",
    "    largest_conn_comp = conn_comps[max(conn_comps.keys())]\n",
    "    Glcc = G.subgraph(largest_conn_comp)\n",
    "    return Glcc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = get_graph(paper_rec_file)\n",
    "G = get_LCC(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4716"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading embeddings from file...: 142009it [00:21, 6482.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings_from_jsonl(embeddings_path, G):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in tqdm(f, desc='reading embeddings from file...'):\n",
    "            line_json = json.loads(line)\n",
    "            if line_json['paper_id'] in G:\n",
    "                embeddings[line_json['paper_id']] = np.array(line_json['embedding'], dtype=np.float32)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_embeddings_from_jsonl(user_activity_and_citations_embeddings_path, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grec = get_LCC(G.subgraph(embeddings.keys()))\n",
    "nx.set_node_attributes(Grec, {node_id: {\"x\":embedding} for node_id, embedding in embeddings.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419, 679, 964)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(Grec), len(Grec.edges), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'rec_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(Grec, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, layer_type, sz_in, num_layers=2, sz_hid=128, sz_out=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # GNN layers with ReLU\n",
    "        encoder = []\n",
    "        encoder.append(layer_type(sz_in, sz_hid))\n",
    "        encoder.append(nn.ReLU())\n",
    "        for _ in range(num_layers-2):\n",
    "            encoder.append(layer_type(sz_hid, sz_hid))\n",
    "            encoder.append(nn.ReLU())\n",
    "        encoder.append(layer_type(sz_hid, sz_out))\n",
    "        self.encoder = nn.ModuleList(encoder)\n",
    "    \n",
    "    # Encoding: usual GNN propagation\n",
    "    def encode(self, fts, adj):\n",
    "        for l in self.encoder:\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                fts = l(fts)\n",
    "            else:\n",
    "                fts = l(fts, adj)\n",
    "        return fts\n",
    "    \n",
    "    # Decoding: dot(H[i], H[j]) for each edge in edge_index\n",
    "    # Larger dot => the model is more confident that this edge should exist\n",
    "    def decode(self, H, edge_index):\n",
    "        return (H[edge_index[0]] * H[edge_index[1]]).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[419, 768], edge_index=[2, 679])\n",
      "tensor([[ 0.0922, -5.5931, -3.3866,  ...,  1.5318, -0.6453, -1.3122],\n",
      "        [-4.3983, -0.8796, -0.8786,  ..., -2.4426, -0.3191, -1.0760],\n",
      "        [-4.6795, -3.8367, -1.8152,  ..., -0.8004, -3.1326, -4.6144],\n",
      "        ...,\n",
      "        [-3.1228, -4.3944, -0.7400,  ..., -3.5155, -0.7761, -0.7719],\n",
      "        [-1.0095,  1.1776, -0.4630,  ...,  2.4056, -3.0469, -1.7069],\n",
      "        [-0.2017, -4.2765, -0.3223,  ..., -1.5384, -1.9732, -4.0335]])\n",
      "Data(x=[419, 768], val_pos_edge_index=[2, 65], test_pos_edge_index=[2, 65], train_pos_edge_index=[2, 392], train_neg_adj_mask=[419, 419], val_neg_edge_index=[2, 65], test_neg_edge_index=[2, 65])\n",
      "\n",
      "Train set: 392 positive edges, we will sample the same number of negative edges at runtime\n",
      "Val set: 65 positive edges, 65 negative edges\n",
      "Test set: 65 positive edges, 65 negative edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/.virtualenvs/main_env/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "data = from_networkx(Grec)\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "print(data)\n",
    "\n",
    "data = train_test_split_edges(data, 0.2, 0.2)\n",
    "print(data.x)\n",
    "print(data)\n",
    "print()\n",
    "print(f'Train set: {data.train_pos_edge_index.shape[1]} positive edges, we will sample the same number of negative edges at runtime')\n",
    "print(f'Val set: {data.val_pos_edge_index.shape[1]} positive edges, {data.val_neg_edge_index.shape[1]} negative edges')\n",
    "print(f'Test set: {data.test_pos_edge_index.shape[1]} positive edges, {data.test_neg_edge_index.shape[1]} negative edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train the given model on the given dataset for num_epochs\n",
    "def train(model, data, num_epochs):\n",
    "    # Set up the loss and the optimizer\n",
    "    loss_fn = nn.BCEWithLogitsLoss() # Binary classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Prepare all edges/labels for val/test\n",
    "    val_pos, val_neg = data.val_pos_edge_index, data.val_neg_edge_index\n",
    "    val_edge_index = torch.cat([val_pos, val_neg], dim=1)\n",
    "    val_labels = torch.cat([torch.ones(val_pos.shape[1]), torch.zeros(val_neg.shape[1])])\n",
    "    \n",
    "    test_pos, test_neg = data.test_pos_edge_index, data.test_neg_edge_index\n",
    "    test_edge_index = torch.cat([test_pos, test_neg], dim=1)\n",
    "    test_labels = torch.cat([torch.ones(test_pos.shape[1]), torch.zeros(test_neg.shape[1])])\n",
    "\n",
    "    # A utility function to compute the ROC-AUC on given edges\n",
    "    def get_roc_auc(model, data, edge_index, labels):\n",
    "        with torch.no_grad():\n",
    "            H = model.encode(data.x, data.train_pos_edge_index)\n",
    "            z = model.decode(H, edge_index)\n",
    "            s = z.sigmoid()\n",
    "            return roc_auc_score(labels, s)\n",
    "\n",
    "    best_auc_val = -1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sample negative edges\n",
    "        pos_edge_index = data.train_pos_edge_index # T_+\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=pos_edge_index, # edges to ignore\n",
    "            num_nodes=data.num_nodes, # N\n",
    "            num_neg_samples=pos_edge_index.shape[1] # number of edges to sample\n",
    "        )\n",
    "    \n",
    "        # Zero grads -> encode to get node latents\n",
    "        optimizer.zero_grad()\n",
    "        H = model.encode(data.x, pos_edge_index)\n",
    "\n",
    "        # Decode to get a score for all (positive and negative) edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "        z = model.decode(H, edge_index)\n",
    "\n",
    "        # Construct the label vector and backprop the loss\n",
    "        labels = torch.cat([torch.ones(pos_edge_index.shape[1]), torch.zeros(neg_edge_index.shape[1])])\n",
    "        \n",
    "        loss = loss_fn(z, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracies, print only if this is the best result so far\n",
    "        auc_val = get_roc_auc(model, data, val_edge_index, val_labels)\n",
    "        auc_test = get_roc_auc(model, data, test_edge_index, test_labels)\n",
    "        if auc_val > best_auc_val:\n",
    "            best_auc_val = auc_val\n",
    "            print(f'[Epoch {epoch+1}/{num_epochs}] Loss: {loss} | Val: {auc_val:.3f} | Test: {auc_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkPredictionModel(\n",
      "  (encoder): ModuleList(\n",
      "    (0): GCNConv(768, 128)\n",
      "    (1): ReLU()\n",
      "    (2): GCNConv(128, 64)\n",
      "  )\n",
      ")\n",
      "[Epoch 1/1000] Loss: 165.4088134765625 | Val: 0.500 | Test: 0.500\n",
      "[Epoch 3/1000] Loss: 119.71524047851562 | Val: 0.517 | Test: 0.584\n"
     ]
    }
   ],
   "source": [
    "model = LinkPredictionModel(torch_geometric.nn.GCNConv, data.x.shape[1])\n",
    "print(model)\n",
    "train(model, data, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d5edf4258bbc637805e302ba85a0073c54e6d301b29d811b35bf8e759b824fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
